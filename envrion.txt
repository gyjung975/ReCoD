conda deactivate; conda env remove -n recocap -y

conda create -n recocap python=3.10 -y && conda activate recocap && pip install --upgrade pip

conda install pytorch==1.12.1 torchvision==0.13.1 torchaudio==0.12.1 cudatoolkit=11.3 -c pytorch -y

pip install -r requirements.txt
pip install git+https://github.com/openai/CLIP.git
python -m spacy download en_core_web_lg

ln -s /home/gyj/NAS/recocap/features features
ln -s /home/gyj/NAS/0datasets/COCO data

# pip install torchmetrics

# https://github.com/explosion/spacy-models
pip3 install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz
python -m spacy download en_core_web_sm
python -m spacy download en_core_web_md
python -m spacy download en_core_web_lg

-----------------------------------------------------

git clone https://github.com/haotian-liu/LLaVA.git
cd LLaVA
conda create -n recocap python=3.10 -y && conda activate recocap && pip install --upgrade pip
pip install -e .
pip install -e ".[train]"
pip install h5py pandas openai spacy matplotlib==3.7.2
pip install git+https://github.com/openai/CLIP.git
python -m spacy download en_core_web_lg
-----------------------------------------------------
1. run_llava - eval_batch(): model.generate
2. lava_llama.py	generate()	return super().generate
    transformers/generation/utils.py	GenerationMixin - generate()		logits_processor
3. transformers/generation/utils.py	GenerationMixin - generate()		return self.sample

*** while True:
4. transformers/generation/utils.py	GenerationMixin - sample()		self()
5. llava_llama.py	LlavaLlamaForCausalLM - forward()		super().forward
6. transformers/models/llama/modeling_llama.py	LlamaForCausalLM - forward()	self.model
	--> OUTPUT logits

    *** loops (multi-layer decoder)
     --> transformers/models/llama/modeling_llama.py - LlamaModel/forward(): decoder_layer
      --> transformers/models/llama/modeling_llama.py - LlamaDecoderLayer/forward(): self.self_attn
       --> transformers/models/llama/modeling_llama.py - LlamaAttention/forward():

transformers/generation/utils.py	GenerationMixin - sample()      logit_processor(logits)
