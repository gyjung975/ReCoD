# ReCoD: Enhancing Image Description for Cross-Modal Understanding via Retrieval and Comparison Feedback Mechanism
<p align="center">
<img src="figs/init_fig.jpg" alt= "" width="" height="250">
</p>

> This repository provides the official implementation of  
> "ReCoD: Enhancing Image Description for Cross-Modal Understanding via Retrieval and Comparison Feedback Mechanism"
> > [Geunyoung Jung](https://gyjung975.github.io/), Jun Park, Hankyeol Lee, [Kyungwoo Song](https://scholar.google.com/citations?user=HWxRii4AAAAJ&hl=ko), and [Jiyoung Jung](https://rcv.uos.ac.kr/)  
> Neurocomputing

## Abstract
<p align="center">
<img src="figs/framework.jpg" alt= "" width="" height="400">
</p>

> To effectively utilize the large language models (LLMs) in the vision domain, it is essential to establish a strong connection between the visual and textual modalities. While deep embeddings can facilitate this connection, representing images as detailed textual descriptions offers significant advantages in terms of the usability and interpretability inherent in natural language. In this paper, we introduce a method of image description enhancement designed to generate highly detailed descriptions that include discriminative attributes of the given image, without requiring additional training. Our method, \textsc{ReCoD}, consists of two main components: 1) \textit{“image retrieval”}, which retrieves the image most similar to the descriptions of the target image, and 2) \textit{“comparison”}, which identifies and describes the differences between the target image and the retrieved image. These two components are complementary and form an iterative feedback mechanism. As this process iterates, the retrieved image becomes visually closer to the target image, and the descriptions become progressively more informative. Extensive experiments demonstrate the effectiveness of bridging the gap between the two modalities and the quality of our enhanced descriptions.

## Setup
```shell
git clone https://github.com/gyjung975/ReCoD.git
cd ReCoD

conda create -n recod python=3.8
conda activate recod

conda install pytorch==1.12.1 torchvision==0.13.1 torchaudio==0.12.1 cudatoolkit=11.3 -c pytorch

pip install -r requirements.txt
pip install git+https://github.com/openai/CLIP.git
```

## Dataset
* Download two datasets [OK-VQA](https://okvqa.allenai.org/) and [A-OKVQA](https://github.com/allenai/aokvqa) under `data/`.
  * All images of both datasets are from [COCO](https://cocodataset.org/#home).
```shell
data
 |── COCO
 |── OK-VQA
 |── A-OKVQA
```

## Datastore
* First, prepare COCO datastore for each dataset under `features/`.
```shell
# COCO14 and COCO17 for OK-VQA and A-OKVQA, respectively
python feature_extract.py --dataset coco --year 2014 --split train
```

## Run
* Generate descriptions using LVLMs w/ and w/o ReCoD.
* Each script is set to the default settings we used in our experiments.
* Output descriptions are automatically stored in a format that enables knowledge-based VQA per loop under `diff/`.

### LLaVA-1.5-13B
```shell
# LLaVA w/o ReCoD
python llava_base.py --data_dir <data/path/> --dataset okvqa
# LLaVA w/ ReCoD (text-based)
python llava_recod.py --data_dir <data/path/> --dataset okvqa
```
### Claude3 (Haiku)
* **Need Claude API key**
* Enter the API key in `claude.py`
```shell
# Claude w/o ReCoD
python claude_base.py --data_dir <data/path/> --dataset okvqa
# Claude w/ ReCoD (text-based)
python claude_recod.py --data_dir <data/path/> --dataset okvqa
# Claude w/ ReCoD (image-based)
python claude_recod_img.py --data_dir <data/path/> --dataset okvqa
```

### Qwen2-VL
```shell
# Claude w/o ReCoD
python qwen_base.py --data_dir <data/path/> --dataset okvqa --size 7
# Claude w/ ReCoD (text-based)
python qwen_recod.py --data_dir <data/path/> --dataset okvqa --size 7
# Claude w/ ReCoD (image-based)
python qwen_recod_img.py --data_dir <data/path/> --dataset okvqa --size 7
```

## Evaluation
We conduct knowledge-based VQA at [PICa](https://github.com/microsoft/PICa).
* GPT API key is required, but other open source LLMs can be used (e.g., LLaMA).
* **To construct in-context learning prompts, naive descriptions of the train set also need to be generated by baseline LVLMs (w/o ReCoD).**

With the results of knowledge-based VQA, the evaluation is done at [VQA](https://github.com/GT-Vision-Lab/VQA) and [A-OKVQA](https://github.com/allenai/aokvqa) for OK-VQA and A-OKVQA, respectively.

## Acknowledgements
Our codes are built upon [LLaVA](https://github.com/haotian-liu/LLaVA), [Qwen2-VL](https://github.com/QwenLM/Qwen2.5-VL), and [PICa](https://github.com/microsoft/PICa). Thanks for their efforts.
